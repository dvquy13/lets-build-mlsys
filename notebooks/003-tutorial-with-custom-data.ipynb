{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqM0hhMuCfzn"
   },
   "source": [
    "# Token classification (PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BTHkBiVCfzp"
   },
   "source": [
    "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "CAi_0hNTCfzp"
   },
   "source": [
    "!pip install datasets evaluate transformers[sentencepiece]\n",
    "!pip install accelerate\n",
    "# To run the training on TPU, you will need to uncomment the following line:\n",
    "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
    "!apt install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyzIlHZICfzp"
   },
   "source": [
    "You will need to setup git, adapt your email and name in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqNtHBZxCfzp"
   },
   "source": [
    "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dZyIk5T8Cfzq"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fad48abcef43728ebeb97b0cf9bd7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aHTw3y28Cfzq"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001ea42629a94822abb08f86670fb0b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/800 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c61bce0ddab45c09048d7d8e3736aa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0f81217967494082079aa1a62dfbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/5.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574ff0eb5b654ba58fcd0d15c2fde170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af8a07079ffd47ec976794a530c17005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"dvquys/restaurant-reviews-public-sources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bTLGN83_Cfzq",
    "outputId": "f65a8cb2-2020-4bdb-b19a-cf27d58584c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'Comments', 'tokens', 'ner_tags'],\n",
       "        num_rows: 24\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['id', 'text', 'Comments', 'tokens', 'ner_tags'],\n",
       "        num_rows: 6\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "jB3u92ewCfzq",
    "outputId": "6975e7d5-fc24-4f51-82f3-07c72aa1b8cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'best',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'experience',\n",
       " 'was',\n",
       " 'knowing',\n",
       " 'that',\n",
       " 'the',\n",
       " 'manager',\n",
       " '(',\n",
       " 'a',\n",
       " 'bubbly',\n",
       " ',',\n",
       " 'friendly',\n",
       " 'young',\n",
       " 'woman',\n",
       " 'with',\n",
       " 'a',\n",
       " 'great',\n",
       " 'smile',\n",
       " ')',\n",
       " 'truly',\n",
       " 'cared',\n",
       " 'about',\n",
       " 'how',\n",
       " 'we',\n",
       " 'were',\n",
       " 'doing',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "i5dJIifACfzq",
    "outputId": "ccb84f29-414f-459a-f12e-b75d6788c0c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0][\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "tQ_vPx_zCfzq",
    "outputId": "e1fea2c6-3d40-4af7-a369-b5ac3ebd79e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(names=['O', 'B-AMBIENCE', 'I-AMBIENCE', 'B-FOOD', 'I-FOOD', 'B-LOCATION', 'I-LOCATION', 'B-PRICE', 'I-PRICE', 'B-SERVICE', 'I-SERVICE'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "ner_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KAIpiIwkCfzq",
    "outputId": "762dcbdc-1346-43ff-bb17-b9b3e5fe84da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-AMBIENCE',\n",
       " 'I-AMBIENCE',\n",
       " 'B-FOOD',\n",
       " 'I-FOOD',\n",
       " 'B-LOCATION',\n",
       " 'I-LOCATION',\n",
       " 'B-PRICE',\n",
       " 'I-PRICE',\n",
       " 'B-SERVICE',\n",
       " 'I-SERVICE']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = ner_feature.feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0xeTcqAMCfzr",
    "outputId": "6ab430b4-8a33-4ec8-c330-dbe9b52fca28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seriously , this is the best   all    you    can    eat    in     town-  As everyone says , the Spicy  Tuna   hand   rolls  are    the    best-  have 4 of these , and you 've broken even . \n",
      "O         O O    O  O   B-FOOD I-FOOD I-FOOD I-FOOD I-FOOD I-FOOD I-FOOD O  O        O    O O   B-FOOD I-FOOD I-FOOD I-FOOD I-FOOD I-FOOD I-FOOD O    O O  O     O O   O   O   O      O    O \n"
     ]
    }
   ],
   "source": [
    "words = raw_datasets[\"train\"][1][\"tokens\"]\n",
    "labels = raw_datasets[\"train\"][1][\"ner_tags\"]\n",
    "line1 = \"\"\n",
    "line2 = \"\"\n",
    "for word, label in zip(words, labels):\n",
    "    full_label = label_names[label]\n",
    "    max_length = max(len(word), len(full_label))\n",
    "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
    "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
    "\n",
    "print(line1)\n",
    "print(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "jNTJWPN0Cfzr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32287de9d6f4e1985a406aeef4cc146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b8c388aeff4706a4964835836591bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c39688e2f34c8e8f8235f07aa22b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60f6fd788dd4f46b37f8706f523637f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8708ec05b3e41f0a7ccd5c55db60aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "95af615CCfzr",
    "outputId": "4bb43d70-35a0-4839-c762-7ce7645b4238"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "CYeL5AGvCfzr",
    "outputId": "228eccc8-d5c4-41fb-84c4-e82d595e3d17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'The',\n",
       " 'best',\n",
       " 'part',\n",
       " 'of',\n",
       " 'the',\n",
       " 'experience',\n",
       " 'was',\n",
       " 'knowing',\n",
       " 'that',\n",
       " 'the',\n",
       " 'manager',\n",
       " '(',\n",
       " 'a',\n",
       " 'b',\n",
       " '##ub',\n",
       " '##bly',\n",
       " ',',\n",
       " 'friendly',\n",
       " 'young',\n",
       " 'woman',\n",
       " 'with',\n",
       " 'a',\n",
       " 'great',\n",
       " 'smile',\n",
       " ')',\n",
       " 'truly',\n",
       " 'cared',\n",
       " 'about',\n",
       " 'how',\n",
       " 'we',\n",
       " 'were',\n",
       " 'doing',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Vi7JvpDuCfzr",
    "outputId": "a4a3dd52-90e4-4f59-8bd5-38342d75c8a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " None]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "8daUmNWECfzr"
   },
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "sJSVH2yPCfzr",
    "outputId": "79074771-98a9-4f45-d458-fa0e3445b8af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-100, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][1][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "dG9xlu1nCfzr"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "suLAZY02Cfzr"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5830504fb24d7688e714ba9d50f3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc721c5b95149b3943ea225cc78f4e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "qk_H33PDCfzr"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "ducHQw9dCfzr",
    "outputId": "b0619874-b40e-4933-ffb3-d6483244ad90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100],\n",
       "        [-100,    0,    0,    0,    0,    0,    3,    4,    4,    4,    4,    4,\n",
       "            4,    4,    0,    0,    0,    0,    0,    3,    4,    4,    4,    4,\n",
       "            4,    4,    4,    4,    4,    4,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0, -100]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "q1Ev204qCfzr",
    "outputId": "f9591b5d-255e-41f4-de6a-29b33888749e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n",
      "[-100, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "-VwXTVu6Cfzr"
   },
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "5l1S0vZ7Cfzr"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14aa9bd8ad39425ba5169e4e58208c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "daundEeLCfzr",
    "outputId": "2d08dd96-08e7-4056-8d2a-57750ea82cc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-FOOD',\n",
       " 'I-FOOD',\n",
       " 'I-FOOD',\n",
       " 'I-FOOD',\n",
       " 'I-FOOD',\n",
       " 'I-FOOD',\n",
       " 'I-FOOD',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-FOOD',\n",
       " 'I-FOOD',\n",
       " 'I-FOOD',\n",
       " 'I-FOOD',\n",
       " 'I-FOOD',\n",
       " 'I-FOOD',\n",
       " 'I-FOOD',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = raw_datasets[\"train\"][1][\"ner_tags\"]\n",
    "labels = [label_names[i] for i in labels]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "o8QgQdU7Cfzr",
    "outputId": "2bb03efb-f9b6-4648-86ae-c9e207e6c022"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FOOD': {'precision': 0.3333333333333333,\n",
       "  'recall': 0.5,\n",
       "  'f1': 0.4,\n",
       "  'number': 2},\n",
       " 'overall_precision': 0.3333333333333333,\n",
       " 'overall_recall': 0.5,\n",
       " 'overall_f1': 0.4,\n",
       " 'overall_accuracy': 0.9714285714285714}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = labels.copy()\n",
    "predictions[8] = \"O\"\n",
    "metric.compute(predictions=[predictions], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "16LU-tC9Cfzr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "fkhYIyAYCfzr"
   },
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "kufpfXaTCfzr"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe020e0889f46a58291ab3111d1771d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "4A0DA7BTCfzr",
    "outputId": "6a26ddb5-1ee7-4779-9330-d19f2667ad5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "cKgKFKfGCfzs"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9ea6144b824cdf97e17df91df2bdd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "oA7M5FxGCfzs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"bert-finetuned-ner\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "7NnI3TghCfzs"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.196855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.068358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.019427</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9, training_loss=2.1065088907877603, metrics={'train_runtime': 7.5618, 'train_samples_per_second': 9.522, 'train_steps_per_second': 1.19, 'total_flos': 1494412415712.0, 'train_loss': 2.1065088907877603, 'epoch': 3.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "FOkeN7DgCfzs",
    "outputId": "512e26c0-f658-46a7-a26e-8b123be17b06"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb1e86bb4cc49409a6b7cb91f694f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/dvquys/bert-finetuned-ner/commit/a8cd60431c394e51f3e48cd886ec733be1037f8d', commit_message='First training complete', commit_description='', oid='a8cd60431c394e51f3e48cd886ec733be1037f8d', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(commit_message=\"First training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "9BQ1HmEOCfzs"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"val\"], collate_fn=data_collator, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "iqZrxcLlCfzs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "FYLV_8ciCfzu"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "UIWm2nX0Cfzu"
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "UIWm2nX0Cfzu"
   },
   "outputs": [],
   "source": [
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "1bXl5hMOCfzu"
   },
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "tgrC6azuCfzu",
    "outputId": "3ec9f22e-0269-4de8-a40d-c46b67a31565"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dvquys/bert-finetuned-ner-accelerate'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "model_name = \"bert-finetuned-ner-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo, repo_exists\n",
    "if not repo_exists(repo_name):\n",
    "    create_repo(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "AUH5cHetCfzu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Cloning https://huggingface.co/dvquys/bert-finetuned-ner-accelerate into local empty directory.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"bert-finetuned-ner-accelerate\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8a4BIBzCfzv"
   },
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "wvojm_THCfzv"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fa48c16e6a453ab31c97e0b9f72535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'accuracy': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'accuracy': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'accuracy': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            key: results[f\"overall_{key}\"]\n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Save and upload\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        # This command might fail without any helpful error messages\n",
    "        # To debug, navigate to the model directory and run `GIT_TRACE=1 GIT_CURL_VERBOSE=1 git push`\n",
    "        # One possible cause is due to we using poetry to install huggingface-cli (since the debug message says huggingface-cli not found)\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        repo.push_to_hub(\n",
    "            commit_message=f\"Training in progress epoch {epoch}\", blocking=False, clean_ok=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'FOOD',\n",
       "  'score': 0.14983541,\n",
       "  'word': 'The',\n",
       "  'start': 0,\n",
       "  'end': 3}]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Local model\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=output_dir, aggregation_strategy=\"simple\", device='cuda'\n",
    ")\n",
    "token_classifier('The place was nice and calm.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "lRD8zHyECfzv",
    "outputId": "8ccc074b-7259-466c-a77b-3017c6b12cf5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f4c6fdcdd04099afb8256f50330cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6d0eb236804ea4af7f2594c0ff18b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/431M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model dvquys/bert-finetuned-ner-accelerate with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForTokenClassification'>, <class 'transformers.models.bert.modeling_bert.BertForTokenClassification'>). See the original errors:\n\nwhile loading with AutoModelForTokenClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n    self._validate_conn(conn)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 1095, in _validate_conn\n    conn.connect()\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/connection.py\", line 652, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/connection.py\", line 805, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/util/ssl_.py\", line 465, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/util/ssl_.py\", line 509, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/ssl.py\", line 501, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/ssl.py\", line 1074, in _create\n    self.do_handshake()\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/ssl.py\", line 1343, in do_handshake\n    self._sslobj.do_handshake()\nsocket.timeout: _ssl.c:1116: The handshake operation timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/requests/adapters.py\", line 667, in send\n    resp = conn.urlopen(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n    retries = retries.increment(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/util/retry.py\", line 474, in increment\n    raise reraise(type(error), error, _stacktrace)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/util/util.py\", line 39, in reraise\n    raise value\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n    response = self._make_request(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 490, in _make_request\n    raise new_e\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 468, in _make_request\n    self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 369, in _raise_timeout\n    raise ReadTimeoutError(\nurllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out. (read timeout=10)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 3428, in from_pretrained\n    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/transformers/utils/hub.py\", line 402, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py\", line 1367, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py\", line 1884, in _download_to_tmp_and_move\n    http_get(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py\", line 459, in http_get\n    r = _request_wrapper(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py\", line 395, in _request_wrapper\n    response = get_session().request(method=method, url=url, **params)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_http.py\", line 66, in send\n    return super().send(request, *args, **kwargs)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/requests/adapters.py\", line 713, in send\n    raise ReadTimeout(e, request=request)\nrequests.exceptions.ReadTimeout: (ReadTimeoutError(\"HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a59b41cd-8e33-491e-a5d9-8b97e63e33e9)')\n\nwhile loading with BertForTokenClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/response.py\", line 748, in _error_catcher\n    yield\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/response.py\", line 894, in _raw_read\n    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\nurllib3.exceptions.IncompleteRead: IncompleteRead(51753489 bytes read, 379182403 more expected)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/requests/models.py\", line 820, in generate\n    yield from self.raw.stream(chunk_size, decode_content=True)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/response.py\", line 1060, in stream\n    data = self.read(amt=amt, decode_content=decode_content)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/response.py\", line 977, in read\n    data = self._raw_read(amt)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/response.py\", line 902, in _raw_read\n    self._fp.close()\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/contextlib.py\", line 137, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/response.py\", line 772, in _error_catcher\n    raise ProtocolError(arg, e) from e\nurllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(51753489 bytes read, 379182403 more expected)', IncompleteRead(51753489 bytes read, 379182403 more expected))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 3428, in from_pretrained\n    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/transformers/utils/hub.py\", line 402, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py\", line 1367, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py\", line 1884, in _download_to_tmp_and_move\n    http_get(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py\", line 539, in http_get\n    for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/requests/models.py\", line 822, in generate\n    raise ChunkedEncodingError(e)\nrequests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(51753489 bytes read, 379182403 more expected)', IncompleteRead(51753489 bytes read, 379182403 more expected))\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Replace this with your own checkpoint\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m repo_name\n\u001b[0;32m----> 5\u001b[0m token_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken-classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimple\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m token_classifier(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe place was nice and calm.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/transformers/pipelines/__init__.py:895\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    894\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 895\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    905\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    906\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py:296\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    295\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 296\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    297\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m         )\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load model dvquys/bert-finetuned-ner-accelerate with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForTokenClassification'>, <class 'transformers.models.bert.modeling_bert.BertForTokenClassification'>). See the original errors:\n\nwhile loading with AutoModelForTokenClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 466, in _make_request\n    self._validate_conn(conn)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 1095, in _validate_conn\n    conn.connect()\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/connection.py\", line 652, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/connection.py\", line 805, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/util/ssl_.py\", line 465, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/util/ssl_.py\", line 509, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/ssl.py\", line 501, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/ssl.py\", line 1074, in _create\n    self.do_handshake()\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/ssl.py\", line 1343, in do_handshake\n    self._sslobj.do_handshake()\nsocket.timeout: _ssl.c:1116: The handshake operation timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/requests/adapters.py\", line 667, in send\n    resp = conn.urlopen(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 843, in urlopen\n    retries = retries.increment(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/util/retry.py\", line 474, in increment\n    raise reraise(type(error), error, _stacktrace)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/util/util.py\", line 39, in reraise\n    raise value\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 789, in urlopen\n    response = self._make_request(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 490, in _make_request\n    raise new_e\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 468, in _make_request\n    self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/connectionpool.py\", line 369, in _raise_timeout\n    raise ReadTimeoutError(\nurllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out. (read timeout=10)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 3428, in from_pretrained\n    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/transformers/utils/hub.py\", line 402, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py\", line 1367, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py\", line 1884, in _download_to_tmp_and_move\n    http_get(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py\", line 459, in http_get\n    r = _request_wrapper(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py\", line 395, in _request_wrapper\n    response = get_session().request(method=method, url=url, **params)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_http.py\", line 66, in send\n    return super().send(request, *args, **kwargs)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/requests/adapters.py\", line 713, in send\n    raise ReadTimeout(e, request=request)\nrequests.exceptions.ReadTimeout: (ReadTimeoutError(\"HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a59b41cd-8e33-491e-a5d9-8b97e63e33e9)')\n\nwhile loading with BertForTokenClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/response.py\", line 748, in _error_catcher\n    yield\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/response.py\", line 894, in _raw_read\n    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\nurllib3.exceptions.IncompleteRead: IncompleteRead(51753489 bytes read, 379182403 more expected)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/requests/models.py\", line 820, in generate\n    yield from self.raw.stream(chunk_size, decode_content=True)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/response.py\", line 1060, in stream\n    data = self.read(amt=amt, decode_content=decode_content)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/response.py\", line 977, in read\n    data = self._raw_read(amt)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/response.py\", line 902, in _raw_read\n    self._fp.close()\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/contextlib.py\", line 137, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/urllib3/response.py\", line 772, in _error_catcher\n    raise ProtocolError(arg, e) from e\nurllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(51753489 bytes read, 379182403 more expected)', IncompleteRead(51753489 bytes read, 379182403 more expected))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/transformers/pipelines/base.py\", line 283, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/transformers/modeling_utils.py\", line 3428, in from_pretrained\n    resolved_archive_file = cached_file(pretrained_model_name_or_path, filename, **cached_file_kwargs)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/transformers/utils/hub.py\", line 402, in cached_file\n    resolved_file = hf_hub_download(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py\", line 1221, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py\", line 1367, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py\", line 1884, in _download_to_tmp_and_move\n    http_get(\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py\", line 539, in http_get\n    for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):\n  File \"/home/dvquys/frostmourne/lets-build-mlsys/.venv/lib/python3.9/site-packages/requests/models.py\", line 822, in generate\n    raise ChunkedEncodingError(e)\nrequests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(51753489 bytes read, 379182403 more expected)', IncompleteRead(51753489 bytes read, 379182403 more expected))\n\n\n"
     ]
    }
   ],
   "source": [
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = repo_name\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\", model=model_checkpoint, aggregation_strategy=\"simple\"\n",
    ")\n",
    "token_classifier('The place was nice and calm.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Token classification (PyTorch)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
